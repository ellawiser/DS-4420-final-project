{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5186bee-bc95-4089-910c-cf1354bcbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lauren Montion & Ella Wiser\n",
    "# DS4420 - Section 3\n",
    "# Implementation of CNN from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e278b9d-e64c-4fee-a69f-cce8b85b1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa7cb1b-e339-4244-b2a3-edac5ee1c338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (462, 112, 112, 1)\n",
      "y_train shape: (462,)\n",
      "x_test shape: (108, 112, 112, 1)\n",
      "y_test shape: (108,)\n",
      "\n",
      "Benign (1): 353 train, 84 test\n",
      "Normal (0): 109 train, 24 test\n"
     ]
    }
   ],
   "source": [
    "# connect to nested folder structure:\n",
    "# Dataset_BUSI_with_GT -> train -> benign\n",
    "#                                  malignant\n",
    "#                                  normal\n",
    "#                         test  -> benign\n",
    "#                                  malignant\n",
    "#                                  normal\n",
    "\n",
    "# connect to path where the data is\n",
    "downloads_path = os.path.expanduser(\"~/Downloads\")\n",
    "dataset_path = os.path.join(downloads_path, \"Dataset_BUSI_with_GT\")\n",
    "\n",
    "# set image size\n",
    "# original sizes all over the place, using 112x112 for efficiency without getting rid of too much information\n",
    "img_size = (112, 112)\n",
    "\n",
    "# class labels, asking \"does this MRI contain a benign tumor or no tumor?\" another way of determining if someone has cancer or not\n",
    "class_labels = {'benign': 1, 'normal': 0}\n",
    "\n",
    "# load in training data\n",
    "train_labels = []\n",
    "train_images = []\n",
    "\n",
    "train_path = os.path.join(dataset_path, 'train')\n",
    "\n",
    "for class_name in ['benign', 'normal']:\n",
    "    class_path = os.path.join(train_path, class_name)\n",
    "    \n",
    "    for img_file in os.listdir(class_path):\n",
    "        if img_file.lower().endswith(('.png')):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            \n",
    "            img = Image.open(img_path).convert('L') \n",
    "            img = img.resize(img_size)\n",
    "            img_array = np.array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=-1)\n",
    "            \n",
    "            train_images.append(img_array)\n",
    "            train_labels.append(class_labels[class_name])\n",
    "\n",
    "# load in test data\n",
    "test_labels = []\n",
    "test_images = []\n",
    "\n",
    "# connects to the test folder within the Dataset_BUSI_with_GT folder\n",
    "test_path = os.path.join(dataset_path, 'test')\n",
    "\n",
    "# grab images in benign and normal folders\n",
    "for class_name in ['benign', 'normal']:\n",
    "    class_path = os.path.join(test_path, class_name)\n",
    "    \n",
    "    for img_file in os.listdir(class_path):\n",
    "        if img_file.lower().endswith(('.png')):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "\n",
    "            # convert to grayscale\n",
    "            img = Image.open(img_path).convert('L') \n",
    "\n",
    "            # resize images to 224x224\n",
    "            img = img.resize(img_size)\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # add channel dimension\n",
    "            img_array = np.expand_dims(img_array, axis=-1) \n",
    "            \n",
    "            test_images.append(img_array)\n",
    "            test_labels.append(class_labels[class_name])\n",
    "\n",
    "# convert to numpy arrays and standardize\n",
    "x_train = np.array(train_images, dtype='float32') / 255.0\n",
    "y_train = np.array(train_labels, dtype='float32')\n",
    "\n",
    "x_test = np.array(test_images, dtype='float32') / 255.0\n",
    "y_test = np.array(test_labels, dtype='float32')\n",
    "\n",
    "# print shapes for future use if there are bugs in CNN\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"\\nBenign (1): {np.sum(y_train == 1)} train, {np.sum(y_test == 1)} test\")\n",
    "print(f\"Normal (0): {np.sum(y_train == 0)} train, {np.sum(y_test == 0)} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba84585d-6850-45ee-8f90-a34687980ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to apply convolutional layers during training - decided to use 3x3 for kernel size\n",
    "class ConvLayers:\n",
    "    def __init__(self, num_kernels, input_channels=1):\n",
    "        self.num_kernels = num_kernels\n",
    "        self.input_channels = input_channels\n",
    "        # kernels shape: (num_kernels, 3, 3, input_channels)\n",
    "        self.kernels = np.random.randn(num_kernels, 3, 3, input_channels) / 9\n",
    "\n",
    "    # using a 3x3 kernel- produces all 3x3 regions the kernel will pass over, NOT using padding\n",
    "    def iterate_regions(self, image):\n",
    "        h, w, c = image.shape\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                im_region = image[i:(i + 3), j:(j + 3), :]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    # forward pass of conv layer\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        batch_size, h, w, c = input.shape\n",
    "        output = np.zeros((batch_size, h - 2, w - 2, self.num_kernels))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for im_region, i, j in self.iterate_regions(input[b]):\n",
    "                for f in range(self.num_kernels):\n",
    "                    output[b, i, j, f] = np.sum(im_region * self.kernels[f])\n",
    "        \n",
    "        return output\n",
    "\n",
    "    # backward pass of conv layer using backpropagation\n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the conv layer.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs (batch, h, w, num_kernels)\n",
    "        - learn_rate is a float.\n",
    "        '''\n",
    "        batch_size = d_L_d_out.shape[0]\n",
    "        d_L_d_kernels = np.zeros(self.kernels.shape)\n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for im_region, i, j in self.iterate_regions(self.last_input[b]):\n",
    "                for f in range(self.num_kernels):\n",
    "                    d_L_d_kernels[f] += d_L_d_out[b, i, j, f] * im_region\n",
    "                    d_L_d_input[b, i:(i+3), j:(j+3), :] += d_L_d_out[b, i, j, f] * self.kernels[f]\n",
    "        \n",
    "        # update kernels\n",
    "        self.kernels -= learn_rate * d_L_d_kernels / batch_size\n",
    "        \n",
    "        return d_L_d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bedcd601-cf27-47c0-8783-a44b15c9d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class so that we can use max pooling to reduce the size of the images in the conv layers, keeps important features\n",
    "class MaxPool:\n",
    "    \n",
    "    # using a 2x2 max pooling layer size- generate 2x2 regions in each image\n",
    "    def iterate_regions(self, image):\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    # forward pass of max pooling layer\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        batch_size, h, w, num_kernels = input.shape\n",
    "        output = np.zeros((batch_size, h // 2, w // 2, num_kernels))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for im_region, i, j in self.iterate_regions(input[b]):\n",
    "                output[b, i, j] = np.amax(im_region, axis=(0, 1))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    # backward pass of max pooling layer\n",
    "    def backprop(self, d_L_d_out):\n",
    "        '''\n",
    "        Performs a backward pass of the maxpool layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs (batch, h/2, w/2, num_kernels)\n",
    "        '''\n",
    "        batch_size = d_L_d_out.shape[0]\n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for im_region, i, j in self.iterate_regions(self.last_input[b]):\n",
    "                h, w, f = im_region.shape\n",
    "                amax = np.amax(im_region, axis=(0, 1))\n",
    "                for i2 in range(h):\n",
    "                    for j2 in range(w):\n",
    "                        for f2 in range(f):\n",
    "                            # If this pixel was the max value, copy the gradient to it.\n",
    "                            if im_region[i2, j2, f2] == amax[f2]:\n",
    "                                d_L_d_input[b, i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[b, i, j, f2]\n",
    "        \n",
    "        return d_L_d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f349a75-ed21-4f8b-9f55-8bc8cb84abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to define a fully connected layer, so that every input maps to every output\n",
    "# will be used once convolution is over and we pass through the NN as nodes\n",
    "class FullyConnect:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) / input_size\n",
    "        self.biases = np.zeros(output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        self.last_input = input\n",
    "        return input @ self.weights + self.biases\n",
    "    \n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        d_L_d_w = self.last_input.T @ d_L_d_out\n",
    "        d_L_d_b = np.sum(d_L_d_out, axis=0)\n",
    "        d_L_d_input = d_L_d_out @ self.weights.T\n",
    "        \n",
    "        self.weights -= learn_rate * d_L_d_w / self.last_input.shape[0]\n",
    "        self.biases -= learn_rate * d_L_d_b / self.last_input.shape[0]\n",
    "        \n",
    "        return d_L_d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe0cdbd-887b-4c10-a8ab-72717d6b7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary activation functions. ReLU to be used in convolution layers and hidden layers of NN\n",
    "# Sigmoid only used in final layer of NN before final output of 1 (benign) or 0 (normal)\n",
    "\n",
    "# adds non-linearity to recognize complex patterns\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.mask = (x > 0)\n",
    "        return x * self.mask\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        return d_out * self.mask\n",
    "\n",
    "# puts values between 0 and 1, useful since this is binary classification task\n",
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        return d_out * self.out * (1 - self.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d26b7e55-78ab-46a5-8a02-126f9652c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross entropy loss, since this is a binary problem. tracked during training\n",
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    eps = 1e-7\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# need derivative to determine in which direction to shift weights during training\n",
    "def binary_cross_entropy_derivative(y_pred, y_true):\n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ff7d57f-bf4a-44e9-b050-177b27192033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model, define components of structure\n",
    "# dimensions included as sanity check\n",
    "\n",
    "# 112x112x1\n",
    "conv1 = ConvLayers(num_kernels=8, input_channels=1)  \n",
    "relu1 = ReLU()\n",
    "\n",
    "# 110x110x8\n",
    "conv2 = ConvLayers(num_kernels=16, input_channels=8) \n",
    "relu2 = ReLU()\n",
    "\n",
    "# 108x108x16\n",
    "conv3 = ConvLayers(num_kernels=32, input_channels=16) \n",
    "relu3 = ReLU()\n",
    "\n",
    "# 106x106x32\n",
    "pool = MaxPool() \n",
    "\n",
    "# dimension of output before flattening: 53x53x32\n",
    "\n",
    "fully_connected_layer1 = FullyConnect(53*53*32, 64) \n",
    "relu4 = ReLU()\n",
    "# now have 89888 nodes\n",
    "\n",
    "fully_connected_layer2 = FullyConnect(64, 1) \n",
    "sigmoid = Sigmoid()\n",
    "# output is 1 node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00503833-8286-41a2-b6c5-d850413092b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN is initialized\n",
      "--- Epoch 1 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.674 | Accuracy: 78%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.672 | Accuracy: 78%\n",
      "--- Epoch 2 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.671 | Accuracy: 77%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.673 | Accuracy: 73%\n",
      "--- Epoch 3 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.667 | Accuracy: 78%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.668 | Accuracy: 75%\n",
      "--- Epoch 4 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.665 | Accuracy: 76%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.663 | Accuracy: 76%\n",
      "--- Epoch 5 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.655 | Accuracy: 81%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.664 | Accuracy: 73%\n",
      "--- Epoch 6 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.656 | Accuracy: 76%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.655 | Accuracy: 76%\n",
      "--- Epoch 7 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.655 | Accuracy: 75%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.652 | Accuracy: 75%\n",
      "--- Epoch 8 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.648 | Accuracy: 75%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.639 | Accuracy: 78%\n",
      "--- Epoch 9 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.640 | Accuracy: 76%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.639 | Accuracy: 75%\n",
      "--- Epoch 10 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.624 | Accuracy: 78%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.624 | Accuracy: 77%\n",
      "--- Epoch 11 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.620 | Accuracy: 76%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.617 | Accuracy: 76%\n",
      "--- Epoch 12 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.596 | Accuracy: 79%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.594 | Accuracy: 77%\n",
      "--- Epoch 13 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.580 | Accuracy: 78%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.596 | Accuracy: 73%\n",
      "--- Epoch 14 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.577 | Accuracy: 76%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.577 | Accuracy: 75%\n",
      "--- Epoch 15 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.566 | Accuracy: 76%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.552 | Accuracy: 76%\n",
      "--- Epoch 16 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.553 | Accuracy: 76%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.541 | Accuracy: 76%\n",
      "--- Epoch 17 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.559 | Accuracy: 74%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.547 | Accuracy: 77%\n",
      "--- Epoch 18 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.501 | Accuracy: 80%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.568 | Accuracy: 73%\n",
      "--- Epoch 19 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.558 | Accuracy: 75%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.493 | Accuracy: 81%\n",
      "--- Epoch 20 ---\n",
      "[Batch 10] Last 10 batches: Average Loss 0.558 | Accuracy: 74%\n",
      "[Batch 20] Last 10 batches: Average Loss 0.539 | Accuracy: 76%\n"
     ]
    }
   ],
   "source": [
    "# begin CNN\n",
    "print('CNN is initialized')\n",
    "\n",
    "# initialize parameters\n",
    "learning_rate = 0.005\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "# train model\n",
    "for epoch in range(epochs):\n",
    "    print(f'--- Epoch {epoch + 1} ---')\n",
    "    \n",
    "    # shuffle training data so model doesn't rely on data order (would make accuracy untrustworthy)\n",
    "    indices = np.random.permutation(len(x_train))\n",
    "    x_train_shuffled = x_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "    \n",
    "    # batch training so computer doesn't get overwhelmed\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for i in range(0, len(x_train_shuffled), batch_size):\n",
    "        batch_x = x_train_shuffled[i:i+batch_size]\n",
    "        batch_y = y_train_shuffled[i:i+batch_size].reshape(-1, 1)\n",
    "        \n",
    "        # forward pass\n",
    "        out = conv1.forward(batch_x)\n",
    "        out = relu1.forward(out)\n",
    "        \n",
    "        out = conv2.forward(out)\n",
    "        out = relu2.forward(out)\n",
    "        \n",
    "        out = conv3.forward(out)\n",
    "        out = relu3.forward(out)\n",
    "        out = pool.forward(out)\n",
    "\n",
    "        # convolution is done, now we flatten and it's a \"normal\" NN\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = fully_connected_layer1.forward(out)\n",
    "        out = relu4.forward(out)\n",
    "        out = fully_connected_layer2.forward(out)\n",
    "        # sigmoid on output since this is a binary problem\n",
    "        out = sigmoid.forward(out)\n",
    "        \n",
    "        # calculate loss and accuracy \n",
    "        training_loss = binary_cross_entropy(out, batch_y)\n",
    "        acc = np.sum((out > 0.5) == batch_y)\n",
    "        \n",
    "        loss += training_loss\n",
    "        num_correct += acc\n",
    "        num_batches += 1\n",
    "        \n",
    "        # backward pass, backpropagation through all 3 layers\n",
    "        gradient = binary_cross_entropy_derivative(out, batch_y)\n",
    "        gradient = sigmoid.backward(gradient)\n",
    "        \n",
    "        gradient = fully_connected_layer2.backprop(gradient, learning_rate)\n",
    "        gradient = relu4.backward(gradient)\n",
    "        \n",
    "        gradient = fully_connected_layer1.backprop(gradient, learning_rate)\n",
    "\n",
    "        # now we \"unflatten\" to go through conv layers\n",
    "        gradient = gradient.reshape(batch_x.shape[0], 53, 53, 32)\n",
    "        gradient = pool.backprop(gradient)\n",
    "        gradient = relu3.backward(gradient)\n",
    "        gradient = conv3.backprop(gradient, learning_rate)\n",
    "        \n",
    "        gradient = relu2.backward(gradient)\n",
    "        gradient = conv2.backprop(gradient, learning_rate)\n",
    "        gradient = relu1.backward(gradient)\n",
    "        \n",
    "        gradient = conv1.backprop(gradient, learning_rate)\n",
    "        \n",
    "        # printing progress every 10 batches for sanity. help\n",
    "        if (num_batches) % 10 == 0:\n",
    "            print(\n",
    "                '[Batch %d] Last 10 batches: Average Loss %.3f | Accuracy: %d%%' %\n",
    "                (num_batches, loss / 10, (num_correct / (10 * batch_size)) * 100)\n",
    "            )\n",
    "            loss = 0\n",
    "            num_correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "376c05b0-fde8-4385-8f09-27d2fee0a80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing the CNN ---\n",
      "Test Loss: 0.6659720516269472\n",
      "Test Accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "# test cnn\n",
    "print('\\n--- Testing the CNN ---')\n",
    "loss = 0\n",
    "num_correct = 0\n",
    "\n",
    "for i in range(0, len(x_test), batch_size):\n",
    "    batch_x = x_test[i:i+batch_size]\n",
    "    batch_y = y_test[i:i+batch_size].reshape(-1, 1)\n",
    "    \n",
    "    # forward pass using same structure as in training\n",
    "    out = conv1.forward(batch_x)\n",
    "    out = relu1.forward(out)\n",
    "    \n",
    "    out = conv2.forward(out)\n",
    "    out = relu2.forward(out)\n",
    "    \n",
    "    out = conv3.forward(out)\n",
    "    out = relu3.forward(out)\n",
    "    out = pool.forward(out)\n",
    "    \n",
    "    out = out.reshape(out.shape[0], -1)\n",
    "    out = fully_connected_layer1.forward(out)\n",
    "    out = relu4.forward(out)\n",
    "    \n",
    "    out = fully_connected_layer2.forward(out)\n",
    "    out = sigmoid.forward(out)\n",
    "    \n",
    "    test_loss = binary_cross_entropy(out, batch_y)\n",
    "    acc = np.sum((out > 0.5) == batch_y)\n",
    "    \n",
    "    loss += test_loss\n",
    "    num_correct += acc\n",
    "\n",
    "num_tests = len(x_test)\n",
    "\n",
    "# print loss and accuracy\n",
    "print('Test Loss:', loss / (num_tests // batch_size))\n",
    "print('Test Accuracy:', (num_correct / num_tests))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
